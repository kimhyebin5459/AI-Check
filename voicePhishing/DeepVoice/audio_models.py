import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras import applications
from tensorflow.keras.layers import (
    Conv2D, MaxPooling2D, Flatten, Dense, Dropout,
    Permute, Concatenate, RepeatVector, Resizing,
    BatchNormalization, Input, Bidirectional, LSTM, Reshape,
    GlobalAveragePooling2D, Average, GlobalAveragePooling1D,
    LayerNormalization, MultiHeadAttention, Layer, GRU
)

from keras.saving import register_keras_serializable

import tensorflow.keras.backend as K

def build_cnn(input_shape=128, dropout=0.5):
    """ CNN ëª¨ë¸ """
    input_layer = Input(shape=(input_shape, 500, 1))

    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)

    # x = Flatten()(x)
    x = GlobalAveragePooling2D()(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(dropout)(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(dropout)(x)
    x = Dense(2, activation='softmax')(x)

    model = Model(inputs=input_layer, outputs=x)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), 
                  loss='categorical_crossentropy', metrics=['accuracy'])
    return model

def build_cnn_bilstm(input_shape=128, dropout=0.5):
    """ CNN + BiLSTM ëª¨ë¸ """
    input_layer = Input(shape=(input_shape, 500, 1))  # 128 (Mel bins), 500 (time frames), 1 (channel)

    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)  # CNN ì¶œë ¥ shape: (batch, ?, ?, 128)

    # âœ… CNNì˜ ì¶œë ¥ í¬ê¸°ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° (ì •ì ì¸ í¬ê¸° ì‚¬ìš©)
    cnn_output_shape = K.int_shape(x)  # (batch, time_steps, feature_dim, channels)
    time_steps = cnn_output_shape[1]   # CNN ì¶œë ¥ í›„ ë‚¨ì€ time_steps
    feature_dim = cnn_output_shape[2]  # CNN ì¶œë ¥ ì±„ë„ (128)
    channels = cnn_output_shape[3]     # CNN ìµœì¢… ì±„ë„ ìˆ˜

    # âœ… Reshape ì ìš© (ì˜¬ë°”ë¥¸ feature_dim ì‚¬ìš©)
    x = Reshape((time_steps, feature_dim * channels))(x)  # batch_size ìœ ì§€, feature_dim * channelsë¡œ ë³€í™˜

    # BiLSTM ì¶”ê°€ (ì–‘ë°©í–¥ LSTM)
    x = Bidirectional(LSTM(128, return_sequences=True))(x)
    x = Bidirectional(LSTM(64))(x)  # ìµœì¢… ì‹œí€€ìŠ¤ ì¶œë ¥

    x = Dense(128, activation='relu')(x)
    x = Dropout(dropout)(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(dropout)(x)
    x = Dense(2, activation='softmax')(x)

    model = Model(inputs=input_layer, outputs=x)
    # model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), 
    #               loss='categorical_crossentropy', metrics=['accuracy'])
    return model


def build_vgg19_bilstm(input_height=142, dropout_rate=0.3):
    """
    VGG19 ê¸°ë°˜ CNN + BiLSTM ëª¨ë¸ êµ¬ì„±
    input_height: Mel + MFCC + Pitchì˜ ì´ feature ì°¨ì› ìˆ˜ (ê¸°ë³¸ 128+13+1 = 142)
    """
    input_layer = Input(shape=(input_height, None, 1))  # (142, time, 1)

    # 1ì±„ë„ì„ 3ì±„ë„ë¡œ í™•ì¥ (VGG19ëŠ” 3ì±„ë„ì„ ìš”êµ¬)
    x = Concatenate(axis=-1)([input_layer, input_layer, input_layer])  # (142, time, 3)

    # ì‹œê°„ì¶•ì„ ê°€ë¡œë¡œ ê°„ì£¼í•´ì„œ VGG19 ì ìš© -> weight=Noneìœ¼ë¡œ ëœë¤ ì´ˆê¸°í™” (transfer learningì€ ì í•©í•˜ì§€ ì•ŠìŒ)
    # resize: VGG19ëŠ” ìµœì†Œ 32x32 ì´ìƒ í•„ìš”í•˜ë¯€ë¡œ timeì¶•ì´ ì‘ìœ¼ë©´ ì˜¤ë¥˜ ë°œìƒ â†’ ì‚¬ì „ reshape ë³´ì¥ í•„ìš”
    x = Permute((2, 1, 3))(x)  # (time, 142, 3)
    x = Resizing(128, 128)(x)  # (224, 224, 3)

    # VGG19 ê°€ì ¸ì˜¤ê¸°
    vgg = applications.VGG19(include_top=False, weights=None, input_shape=(128, 128, 3))
    x = vgg(x)
    x = GlobalAveragePooling2D()(x)

    # BiLSTM ì²˜ë¦¬ (ì„ë² ë”©ì²˜ëŸ¼ ì·¨ê¸‰)
    x = RepeatVector(1)(x)  # (1, feature_dim)
    x = Bidirectional(LSTM(128, return_sequences=False))(x)

    # ë¶„ë¥˜ê¸°
    x = Dropout(dropout_rate)(x)
    output = Dense(2, activation='softmax')(x)

    return Model(inputs=input_layer, outputs=output)

def build_vgg19_bilstm_ensemble(input_shape=(299, None, 1), dropout_rate=0.3):
    """
    VGG19 ê¸°ë°˜ CNNê³¼ BiLSTM ê¸°ë°˜ ëª¨ë¸ì„ ì•™ìƒë¸”(soft voting)í•œ ëª¨ë¸
    - VGG19ëŠ” (142, time, 1) â†’ (128, 128, 3) ë¡œ reshape í›„ ì‚¬ìš©
    - BiLSTMì€ ì‹œê³„ì—´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - ë‘ ëª¨ë¸ì˜ softmax ì¶œë ¥ì„ í‰ê·  (Soft Voting)
    """

    # â¬› Input
    input_layer = Input(shape=input_shape)

    ### ğŸ§  VGG19 Branch ###
    x_vgg = Concatenate(axis=-1)([input_layer, input_layer, input_layer])  # (142, time, 3)
    x_vgg = Permute((2, 1, 3))(x_vgg)  # (time, 142, 3)
    x_vgg = Resizing(128, 128)(x_vgg)

    vgg = applications.VGG19(include_top=False, weights=None, input_shape=(128, 128, 3))
    x_vgg = vgg(x_vgg)
    x_vgg = GlobalAveragePooling2D()(x_vgg)
    x_vgg = RepeatVector(1)(x_vgg)
    x_vgg = Bidirectional(LSTM(128, return_sequences=False))(x_vgg)
    x_vgg = Dropout(dropout_rate)(x_vgg)
    out_vgg = Dense(2, activation='softmax')(x_vgg)

    ### ğŸ§  BiLSTM Branch ###
    x_rnn = Permute((2, 1, 3))(input_layer)  # (time, 142, 1)
    x_rnn = Reshape((-1, input_shape[0]))(x_rnn)  # (time, 142)
    x_rnn = Bidirectional(LSTM(128, return_sequences=True))(x_rnn)
    x_rnn = Bidirectional(LSTM(64))(x_rnn)
    x_rnn = Dropout(dropout_rate)(x_rnn)
    out_rnn = Dense(2, activation='softmax')(x_rnn)

    ### ğŸ§  Soft Voting ì•™ìƒë¸” ###
    averaged_output = Average()([out_vgg, out_rnn])

    model = Model(inputs=input_layer, outputs=averaged_output)
    return model


# def non_cudnn_bilstm(units, return_sequences):
#     return Bidirectional(LSTM(
#         units,
#         return_sequences=return_sequences,
#         activation='tanh',
#         recurrent_activation='sigmoid',
#         use_bias=True,
#         unroll=False,
#         recurrent_dropout=0.0
#     ))

def non_cudnn_bilstm(units, return_sequences):
    return Bidirectional(LSTM(
        units,
        return_sequences=return_sequences,
        activation='tanh',
        recurrent_activation='sigmoid',
        use_bias=True,
        kernel_initializer='glorot_uniform',
        recurrent_initializer='orthogonal',
        unit_forget_bias=True,
        dropout=0.0,
        recurrent_dropout=0.0,
        implementation=2  # âœ… CuDNN ë¹„í™œì„±í™” ê°•ì œ
    ))


def build_vgg19_bilstm_ensemble_cpu_lstm(input_shape=(299, None, 1), dropout_rate=0.3):
    """
    CuDNN ì—†ì´ ì¼ë°˜ LSTM ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì •ëœ VGG19 + BiLSTM ì•™ìƒë¸” ëª¨ë¸
    """

    input_layer = Input(shape=input_shape)

    ### ğŸ§  VGG19 Branch ###
    x_vgg = Concatenate(axis=-1)([input_layer, input_layer, input_layer])  # (299, None, 3)
    x_vgg = Permute((2, 1, 3))(x_vgg)  # (time, 299, 3)
    x_vgg = Resizing(128, 128)(x_vgg)

    vgg = applications.VGG19(include_top=False, weights=None, input_shape=(128, 128, 3))
    x_vgg = vgg(x_vgg)
    x_vgg = GlobalAveragePooling2D()(x_vgg)
    x_vgg = RepeatVector(1)(x_vgg)
    x_vgg = non_cudnn_bilstm(128, return_sequences=False)(x_vgg)
    x_vgg = Dropout(dropout_rate)(x_vgg)
    out_vgg = Dense(2, activation='softmax')(x_vgg)

    ### ğŸ§  BiLSTM Branch ###
    x_rnn = Permute((2, 1, 3))(input_layer)  # (time, 299, 1)
    x_rnn = Reshape((-1, input_shape[0]))(x_rnn)  # (time, 299)
    x_rnn = non_cudnn_bilstm(128, return_sequences=True)(x_rnn)
    x_rnn = non_cudnn_bilstm(64, return_sequences=False)(x_rnn)
    x_rnn = Dropout(dropout_rate)(x_rnn)
    out_rnn = Dense(2, activation='softmax')(x_rnn)

    ### ğŸ§  Soft Voting ì•™ìƒë¸” ###
    averaged_output = Average()([out_vgg, out_rnn])

    model = Model(inputs=input_layer, outputs=averaged_output)
    return model


@register_keras_serializable()
class TransformerEncoderBlock(tf.keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):  # <- í•µì‹¬
        super().__init__(**kwargs)  # <- trainable, dtype ë“± ì²˜ë¦¬
        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential([
            tf.keras.layers.Dense(ff_dim, activation="relu"),
            tf.keras.layers.Dense(embed_dim),
        ])
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)

    def call(self, inputs, training=False):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

    def get_config(self):
        config = super().get_config()
        config.update({
            "embed_dim": self.att.key_dim,
            "num_heads": self.att.num_heads,
            "ff_dim": self.ffn.layers[0].units,
            "rate": self.dropout1.rate,
        })
        return config



def build_cnn_transformer(input_shape=(128, 500, 1), dropout=0.3, num_heads=4, ff_dim=256):
    """
    CNN + Transformer ê¸°ë°˜ ë”¥ë³´ì´ìŠ¤ íƒì§€ ëª¨ë¸
    """
    inputs = Input(shape=input_shape)

    # âœ… CNN Block
    x = Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)  # â†’ (batch, t', f', 128)

    # # âœ… Reshape to (batch, time, embed_dim)
    # t_dim = tf.shape(x)[1]
    # f_dim = tf.shape(x)[2]
    # c_dim = x.shape[-1]
    # x = Reshape((-1, x.shape[2] * x.shape[3]))(x)  # Flatten (time, f'*c) â†’ token sequence

    # âœ… ì˜¬ë°”ë¥¸ ì½”ë“œ (KerasTensorì—ì„œ ì •ì  shape ì¶”ì¶œ)
    cnn_output_shape = K.int_shape(x)  # returns tuple like (batch_size, t', f', c)
    t_dim = cnn_output_shape[1]
    f_dim = cnn_output_shape[2]
    c_dim = cnn_output_shape[3]
    
    # reshape for Transformer input
    x = Reshape((t_dim, f_dim * c_dim))(x)

    # âœ… Transformer Block
    transformer_block = TransformerEncoderBlock(embed_dim=x.shape[-1], num_heads=num_heads, ff_dim=ff_dim, rate=dropout)
    x = transformer_block(x)

    x = GlobalAveragePooling1D()(x)  # sequence â†’ vector

    x = Dense(128, activation='relu')(x)
    x = Dropout(dropout)(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(dropout)(x)
    outputs = Dense(2, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

def build_vgg19_bigru_ensemble(input_shape=(171, None, 1), dropout_rate=0.3):
    """
    VGG19 ê¸°ë°˜ CNNê³¼ BiGRU ê¸°ë°˜ ëª¨ë¸ì„ ì•™ìƒë¸”(soft voting)
    - VGG19ëŠ” ì‹œê°ì  íŠ¹ì§•
    - GRUëŠ” ì‹œê³„ì—´ íŒ¨í„´
    - ë‘˜ì˜ ì¶œë ¥ì„ softmax í›„ í‰ê· 
    """

    # â¬› Input
    input_layer = Input(shape=input_shape)  # (171, time, 1)

    ### ğŸ§  VGG19 Branch ###
    x_vgg = Concatenate(axis=-1)([input_layer, input_layer, input_layer])  # (171, time, 3)
    x_vgg = Permute((2, 1, 3))(x_vgg)  # (time, 171, 3)
    x_vgg = Resizing(128, 128)(x_vgg)

    vgg = applications.VGG19(include_top=False, weights=None, input_shape=(128, 128, 3))
    x_vgg = vgg(x_vgg)
    x_vgg = GlobalAveragePooling2D()(x_vgg)
    x_vgg = RepeatVector(1)(x_vgg)
    x_vgg = Bidirectional(GRU(128, return_sequences=False))(x_vgg)
    x_vgg = Dropout(dropout_rate)(x_vgg)
    out_vgg = Dense(2, activation='softmax')(x_vgg)

    ### ğŸ§  GRU Branch ###
    x_rnn = Permute((2, 1, 3))(input_layer)  # (time, 171, 1)
    x_rnn = Reshape((-1, input_shape[0]))(x_rnn)  # (time, 171)
    x_rnn = Bidirectional(GRU(128, return_sequences=True))(x_rnn)
    x_rnn = Bidirectional(GRU(64))(x_rnn)
    x_rnn = Dropout(dropout_rate)(x_rnn)
    out_rnn = Dense(2, activation='softmax')(x_rnn)

    ### ğŸ§  Soft Voting ì•™ìƒë¸” ###
    averaged_output = Average()([out_vgg, out_rnn])

    model = Model(inputs=input_layer, outputs=averaged_output)
    return model

def build_img_cnn_bigru_ensemble(input_shape=(128, 500, 1), dropout_rate=0.3):
    """
    ì´ë¯¸ì§€ ê¸°ë°˜ CNN + ì‹œê³„ì—´ ê¸°ë°˜ BiGRU ì•™ìƒë¸” ëª¨ë¸
    - CNNì€ ì‹œê°ì  íŠ¹ì§• ì¶”ì¶œ
    - GRUëŠ” ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ
    - ë‘ ë¶„ê¸°ì˜ softmax ì¶œë ¥ì„ í‰ê·  (soft voting)
    """
    input_layer = Input(shape=input_shape)  # (128, 500, 1)

    ### ğŸ§  CNN Branch ###
    x_cnn = Conv2D(32, (3, 3), padding='same', activation='relu')(input_layer)
    x_cnn = BatchNormalization()(x_cnn)
    x_cnn = MaxPooling2D((2, 2))(x_cnn)

    x_cnn = Conv2D(64, (3, 3), padding='same', activation='relu')(x_cnn)
    x_cnn = BatchNormalization()(x_cnn)
    x_cnn = MaxPooling2D((2, 2))(x_cnn)

    x_cnn = Conv2D(128, (3, 3), padding='same', activation='relu')(x_cnn)
    x_cnn = BatchNormalization()(x_cnn)
    x_cnn = MaxPooling2D((2, 2))(x_cnn)

    x_cnn = GlobalAveragePooling2D()(x_cnn)
    x_cnn = Dense(128, activation='relu')(x_cnn)
    x_cnn = Dropout(dropout_rate)(x_cnn)
    out_cnn = Dense(2, activation='softmax')(x_cnn)

    ### ğŸ§  BiGRU Branch ###
    x_rnn = Permute((2, 1, 3))(input_layer)  # (time, features, 1)
    x_rnn = Reshape((-1, input_shape[0]))(x_rnn)  # (time, features)
    x_rnn = Bidirectional(GRU(128, return_sequences=True))(x_rnn)
    x_rnn = Bidirectional(GRU(64))(x_rnn)
    x_rnn = Dropout(dropout_rate)(x_rnn)
    out_rnn = Dense(2, activation='softmax')(x_rnn)

    ### ğŸ§  Soft Voting ì•™ìƒë¸” ###
    averaged_output = Average()([out_cnn, out_rnn])

    model = Model(inputs=input_layer, outputs=averaged_output)
    return model

from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D,
    Dense, Dropout, BatchNormalization, Concatenate
)

def build_multifeature_cnn_model(image_shape=(128, 500, 1), vector_shape=(10,), dropout_rate=0.3):
    """
    âœ… ê²½ëŸ‰ CNN + ë²¡í„° feature ë³‘í•© ëª¨ë¸
    - ì´ë¯¸ì§€ ì…ë ¥ì€ ê°„ë‹¨í•œ CNN êµ¬ì¡°ë¡œ ì²˜ë¦¬
    - ë²¡í„° ì…ë ¥ì€ Dense ì²˜ë¦¬
    - ì¤‘ê°„ì—ì„œ featureë¥¼ concatí•˜ì—¬ ë¶„ë¥˜ê¸°ë¡œ ì „ë‹¬
    """

    # ğŸŸ¦ ì´ë¯¸ì§€ ì…ë ¥ ë¶„ê¸° (ê²½ëŸ‰ CNN)
    image_input = Input(shape=image_shape, name="image_input")
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(image_input)  # ì±„ë„ ìˆ˜ ì¶•ì†Œ
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)

    x = GlobalAveragePooling2D()(x)

    # ğŸŸ§ ë²¡í„° ì…ë ¥ ë¶„ê¸° (Dense)
    vector_input = Input(shape=vector_shape, name="vector_input")
    v = Dense(64, activation='relu')(vector_input)
    v = Dropout(dropout_rate)(v)

    # ğŸ”— ë³‘í•©
    x = Concatenate()([x, v])
    x = Dense(64, activation='relu')(x)
    x = Dropout(dropout_rate)(x)

    # âœ… ì¶œë ¥ì¸µ
    output = Dense(2, activation='softmax')(x)

    return Model(inputs=[image_input, vector_input], outputs=output)


def build_multifeature_cnn_bilstm_ensemble(image_shape=(128, 500, 1), vector_shape=(10,), dropout_rate=0.3):
    """
    âœ… ì´ë¯¸ì§€(CNN) + ë²¡í„°(Dense) + ì˜¤ë””ì˜¤ ì‹œê³„ì—´(BiLSTM) ì•™ìƒë¸” ëª¨ë¸
    - ì´ë¯¸ì§€ì™€ ë²¡í„°ëŠ” CNN + Dense êµ¬ì¡°ë¡œ ë³‘í•©
    - ì˜¤ë””ì˜¤ ì‹œí€€ìŠ¤ëŠ” BiLSTMìœ¼ë¡œ ì²˜ë¦¬
    - ë‘ ë¶„ê¸° ì¶œë ¥ í‰ê·  (Soft Voting)
    """

    # ğŸŸ¦ ì´ë¯¸ì§€ ì…ë ¥ (CNN)
    image_input = Input(shape=image_shape, name="image_input")
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(image_input)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)
    x = GlobalAveragePooling2D()(x)

    # ğŸŸ§ ë²¡í„° ì…ë ¥ (Dense)
    vector_input = Input(shape=vector_shape, name="vector_input")
    v = Dense(64, activation='relu')(vector_input)
    v = Dropout(dropout_rate)(v)

    # ğŸ”— CNN + ë²¡í„° ë³‘í•©
    combined = Concatenate()([x, v])
    combined = Dense(64, activation='relu')(combined)
    combined = Dropout(dropout_rate)(combined)
    out_combined = Dense(2, activation='softmax')(combined)

    # ğŸ§ ì˜¤ë””ì˜¤ ì‹œí€€ìŠ¤ ì…ë ¥ (Mel+ë²¡í„° feature ì‹œí€€ìŠ¤)
    # ê°€ì •: audio_seq_shape = (time, feature) í˜•íƒœë¡œ ì •ê·œí™”ë¨
    audio_seq_input = Input(shape=(400, 128), name="audio_sequence_input")  # ì˜ˆì‹œ
    rnn = Bidirectional(LSTM(64, implementation=1, return_sequences=True))(audio_seq_input)
    rnn = Bidirectional(LSTM(32, implementation=1))(rnn)
    rnn = Dropout(dropout_rate)(rnn)
    out_rnn = Dense(2, activation='softmax')(rnn)

    # ğŸ§  Soft voting ì•™ìƒë¸”
    from tensorflow.keras.layers import Average
    final_output = Average()([out_combined, out_rnn])

    return Model(inputs=[image_input, vector_input, audio_seq_input], outputs=final_output)

def build_multifeature_cnn_bigru_ensemble(image_shape=(128, 500, 1), vector_shape=(10,), dropout_rate=0.3):
    """
    âœ… ì´ë¯¸ì§€(CNN) + ë²¡í„°(Dense) + ì˜¤ë””ì˜¤ ì‹œê³„ì—´(BiLSTM) ì•™ìƒë¸” ëª¨ë¸
    - ì´ë¯¸ì§€ì™€ ë²¡í„°ëŠ” CNN + Dense êµ¬ì¡°ë¡œ ë³‘í•©
    - ì˜¤ë””ì˜¤ ì‹œí€€ìŠ¤ëŠ” BiLSTMìœ¼ë¡œ ì²˜ë¦¬
    - ë‘ ë¶„ê¸° ì¶œë ¥ í‰ê·  (Soft Voting)
    """

    # ğŸŸ¦ ì´ë¯¸ì§€ ì…ë ¥ (CNN)
    image_input = Input(shape=image_shape, name="image_input")
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(image_input)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)
    x = GlobalAveragePooling2D()(x)

    # ğŸŸ§ ë²¡í„° ì…ë ¥ (Dense)
    vector_input = Input(shape=vector_shape, name="vector_input")
    v = Dense(64, activation='relu')(vector_input)
    v = Dropout(dropout_rate)(v)

    # ğŸ”— CNN + ë²¡í„° ë³‘í•©
    combined = Concatenate()([x, v])
    combined = Dense(64, activation='relu')(combined)
    combined = Dropout(dropout_rate)(combined)
    out_combined = Dense(2, activation='softmax')(combined)

    # ğŸ§ ì˜¤ë””ì˜¤ ì‹œí€€ìŠ¤ ì…ë ¥ (Mel+ë²¡í„° feature ì‹œí€€ìŠ¤)
    # ê°€ì •: audio_seq_shape = (time, feature) í˜•íƒœë¡œ ì •ê·œí™”ë¨
    audio_seq_input = Input(shape=(400, 128), name="audio_sequence_input")  # ì˜ˆì‹œ
    # rnn = Bidirectional(GRU(64, implementation=1, return_sequences=True))(audio_seq_input)
    # rnn = Bidirectional(GRU(32, implementation=1))(rnn)
    rnn = Bidirectional(GRU(64, return_sequences=True, reset_after=False, recurrent_activation='sigmoid'))(audio_seq_input)
    rnn = Bidirectional(GRU(32, reset_after=False, recurrent_activation='sigmoid'))(rnn)
    rnn = Dropout(dropout_rate)(rnn)
    out_rnn = Dense(2, activation='softmax')(rnn)

    # ğŸ§  Soft voting ì•™ìƒë¸”
    from tensorflow.keras.layers import Average
    final_output = Average()([out_combined, out_rnn])

    return Model(inputs=[image_input, vector_input, audio_seq_input], outputs=final_output)

def build_multifeature_cnn_bilstm_concat_ensemble(image_shape=(128, 400, 1), vector_shape=(10,), dropout_rate=0.3):
    """
    âœ… ì´ë¯¸ì§€(CNN) + ë²¡í„°(Dense) + ì˜¤ë””ì˜¤ ì‹œí€€ìŠ¤(BiLSTM) ë¶„ê¸° ê²°í•© í›„,
    Concatí•˜ì—¬ Denseë¡œ ìµœì¢… íŒë‹¨í•˜ëŠ” ì•™ìƒë¸” êµ¬ì¡°
    """

    # ğŸŸ¦ ì´ë¯¸ì§€ ì…ë ¥ (CNN)
    image_input = Input(shape=image_shape, name="image_input")
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(image_input)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)
    x = GlobalAveragePooling2D()(x)

    # ğŸŸ§ ë²¡í„° ì…ë ¥ (Dense)
    vector_input = Input(shape=vector_shape, name="vector_input")
    v = Dense(64, activation='relu')(vector_input)
    v = Dropout(dropout_rate)(v)

    # ğŸ”— CNN + ë²¡í„° ë³‘í•©
    combined = Concatenate()([x, v])
    combined = Dense(64, activation='relu')(combined)
    combined = Dropout(dropout_rate)(combined)
    out_combined = Dense(32, activation='relu')(combined)  # ğŸ‘‰ Softmax ëŒ€ì‹  ì¤‘ê°„ ì¶œë ¥

    # ğŸ§ ì˜¤ë””ì˜¤ ì‹œí€€ìŠ¤ ì…ë ¥ (Mel ì‹œí€€ìŠ¤)
    audio_seq_input = Input(shape=(400, 128), name="audio_sequence_input")
    # rnn = Bidirectional(LSTM(64, return_sequences=True))(audio_seq_input)
    # rnn = Bidirectional(LSTM(32))(rnn)
    # after (ìˆœìˆ˜ TF ì—°ì‚°ìœ¼ë¡œ ë³€ê²½)
    rnn = Bidirectional(LSTM(64, return_sequences=True, activation='tanh', recurrent_activation='sigmoid', unroll=True))(audio_seq_input)
    rnn = Bidirectional(LSTM(32, activation='tanh', recurrent_activation='sigmoid', unroll=True))(rnn)
    rnn = Dropout(dropout_rate)(rnn)
    out_rnn = Dense(32, activation='relu')(rnn)  # ğŸ‘‰ Softmax ëŒ€ì‹  ì¤‘ê°„ ì¶œë ¥

    # ğŸ§  Concat í›„ ìµœì¢… íŒë‹¨
    merged = Concatenate()([out_combined, out_rnn])  # (64,)
    final_output = Dense(2, activation='softmax')(merged)

    return Model(inputs=[image_input, vector_input, audio_seq_input], outputs=final_output)


def convert_model_to_tflite_with_dynamic_quantization(model):
    """
    ë™ì  ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ TensorFlow Lite í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    """
    # ëª¨ë¸ì„ TensorFlow Liteë¡œ ë³€í™˜
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    
    # ë™ì  ì–‘ìí™” ì„¤ì •
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]

    # ì–‘ìí™”ëœ ëª¨ë¸ë¡œ ë³€í™˜
    tflite_model = converter.convert()

    return tflite_model