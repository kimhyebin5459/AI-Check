import numpy as np
import librosa
import tensorflow as tf
import os
import glob
from utils.load import load_audio_file_paths

os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "7"
print("ğŸ–¥ï¸ Physical GPUs:", tf.config.list_physical_devices('GPU'))

# âœ… **ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ**
model_path = "model/CNN_BiLSTM_best_model_2025-03-24_05-31-37.keras"
model = tf.keras.models.load_model(model_path, compile=False)

# âœ… **ì˜¤ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜ (í•™ìŠµ ì‹œì™€ ë™ì¼í•˜ê²Œ)**
def extract_audio_features(file_path, feature_type='mel', max_len=500):
    y, sr = librosa.load(file_path, sr=16000)

    # ğŸ“Œ **ë„ˆë¬´ ì§§ì€ ì˜¤ë””ì˜¤ëŠ” ë¬´ì‹œ**
    if len(y) < 1600:  # 0.1ì´ˆ ë¯¸ë§Œì´ë©´ ìŠ¤í‚µ
        print(f"âš  Warning: File {file_path} is too short and skipped.")
        return None

    # ğŸ“Œ **ë°ì´í„° ì¦ê°• (Inferenceì—ì„œëŠ” ì œì™¸ ê°€ëŠ¥)**
    # y = librosa.effects.time_stretch(y, rate=np.random.uniform(0.8, 1.2))  # í•™ìŠµ ì‹œë§Œ ì‚¬ìš©

    # ì ì ˆí•œ n_fft ì„¤ì •
    n_fft = min(len(y), 2048) if len(y) >= 256 else 256

    if feature_type == 'mel':
        feature = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, n_fft=n_fft, hop_length=512, fmax=8000)
        feature = librosa.power_to_db(feature, ref=np.max)

    # ğŸ“Œ **íŒ¨ë”© ì²˜ë¦¬**
    pad_width = max_len - feature.shape[1]
    if pad_width > 0:
        feature = np.pad(feature, ((0, 0), (0, pad_width)), mode='constant')
    else:
        feature = feature[:, :max_len]

    return feature

# âœ… **Inference ì‹¤í–‰ í•¨ìˆ˜**
def predict_audio(file_path):
    feature = extract_audio_features(file_path)
    if feature is None:
        return None  # ì§§ì€ íŒŒì¼ ë¬´ì‹œ

    # ğŸ“Œ **ì…ë ¥ ì°¨ì› ë³€í™˜ (ëª¨ë¸ê³¼ ë™ì¼í•œ í˜•íƒœë¡œ)**
    feature = feature[np.newaxis, ..., np.newaxis]  # (1, 128, max_len, 1)

    # ğŸ“Œ **ëª¨ë¸ ì˜ˆì¸¡**
    prediction = model.predict(feature)
    predicted_class = np.argmax(prediction, axis=1)[0]
    confidence = np.max(prediction)

    label_map = {0: "âœ… Real Voice", 1: "â€¼ï¸ Deepfake Voice"}
    print(f"ğŸ™ {file_path} â†’ Predicted: {label_map[predicted_class]} (Confidence: {confidence:.2%})")

    return predicted_class, confidence

# âœ… **í…ŒìŠ¤íŠ¸í•  ì˜¤ë””ì˜¤ íŒŒì¼ ì§€ì •**
test_audio_files = load_audio_file_paths("dataset/audio/tmp")

# âœ… **ëª¨ë“  í…ŒìŠ¤íŠ¸ íŒŒì¼ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰**
for file in test_audio_files:
    predict_audio(file)