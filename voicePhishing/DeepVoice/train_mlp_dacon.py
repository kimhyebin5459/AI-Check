# mlp_strong_train.py
import os
import pandas as pd
import numpy as np
import joblib
import json
import datetime
import tensorflow as tf

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score
from sklearn.utils.class_weight import compute_class_weight


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau

# from utils.extract_features import get_feature_names_v3
# from utils.extract_features_v4 import get_feature_names_v4
from utils.extract_features_v5 import get_feature_names_v5
from utils.extract_features_v6 import get_feature_names_v6
from utils.extract_features_v7 import get_feature_names_v7
from utils.extract_features_v8 import get_feature_names_v8
from utils.extract_features_v9 import get_feature_names_v9

# âœ… GPU ì„¤ì •
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "9"
print("ğŸ–¥ï¸ Physical GPUs:", tf.config.list_physical_devices('GPU'))

# âœ… ì„¤ì •
class_weight = "none"
model_name = "dacon-mlp-256-128-64-v9-auc-2-1"
# model_name = "mlp-256-128-64-v5-auc-2-1"
# model_name = "mlp-256-128-v4-recall"
learning_rate = 0.001
epoch = 100
batch_size = 32
SEED = 42

# âœ… ë””ë ‰í† ë¦¬ ìƒì„±
today = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
log_dir = f"logs/{today}"
os.makedirs("logs", exist_ok=True)
os.makedirs("ckpt", exist_ok=True)
os.makedirs("model", exist_ok=True)

# âœ… ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# df = pd.read_csv("dataset/final_url_dataset_with_features.csv").dropna()
# df = pd.read_csv("dataset/final_url_dataset_with_features_v4.csv").dropna()
# df = pd.read_csv("dataset/final_url_dataset_with_features_v5.csv").dropna()
df = pd.read_csv("dataset/train_dataset_with_features_v9.csv").dropna()
# df = pd.read_csv("dataset/final_url_dataset_with_features_v3.csv").dropna()

# âœ… ì–¸ë”ìƒ˜í”Œë§: label 1 ìˆ˜ì— ë§ì¶° label 0 ìƒ˜í”Œë§
count_1 = sum(df["label"] == 1)
df_1 = df[df["label"] == 1]
# df_0 = df[df["label"] == 0].sample(n=round(count_1*1.5), random_state=SEED)
df_0 = df[df["label"] == 0].sample(n=round(count_1*2), random_state=SEED)

df_balanced = pd.concat([df_1, df_0]).sample(frac=1, random_state=SEED)

# X = df_balanced.drop(columns=["url", "domain", "label"])
# y = df_balanced["label"]

# selected_features = get_feature_names_v5()
selected_features = get_feature_names_v9()
X = df_balanced[selected_features]
y = df_balanced["label"]


# âœ… log1p ë³€í™˜
log_transform_cols = [
    "url_len",
    "digit_count",
    "subdomain_count",
    "hostname_len"
]

# âœ… ë³€í™˜ ë° ìƒˆë¡œìš´ ì—´ë¡œ ì¶”ê°€
for col in selected_features:
    if col in df.columns:
        df[col] = np.log1p(df[col])

# âœ… train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


# âœ… y_testë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
y_test_df = pd.DataFrame(y_test, columns=["label"])
# âœ… X_testì— label ì»¬ëŸ¼ì„ ì¶”ê°€í•˜ì—¬ ì €ì¥
X_test_with_label = X_test.copy()
X_test_with_label["label"] = y_test_df.values

# âœ… CSVë¡œ ì €ì¥
X_test_with_label.to_csv("dataset/dacon_test_split_only_v9.csv", index=False)
print("âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: dataset/dacon_test_split_only_v9.csv")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

scaler_dict = {
    "mean": scaler.mean_.tolist(),
    "scale": scaler.scale_.tolist()
}


joblib.dump(scaler, f"scaler/scaler_{model_name}.pkl")
with open(f"scaler/scaler_params_{model_name}.json", "w") as f:
    json.dump(scaler_dict, f)
    
print("ğŸ” NaN in X:", np.isnan(X_train_scaled).sum())
print("ğŸ” NaN in y:", np.isnan(y_train).sum())

# âœ… MLP ëª¨ë¸ ì •ì˜
model = Sequential([
    Dense(256, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    Dropout(0.4),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    # Dropout(0.3),
    # Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer=Adam(learning_rate=learning_rate),
    loss="binary_crossentropy",
    metrics=[
        tf.keras.metrics.BinaryAccuracy(name="accuracy"),
        tf.keras.metrics.Recall(name="recall"),
        tf.keras.metrics.AUC(name="auc")
    ]
)

checkpoint_cb = ModelCheckpoint(
    filepath=f"ckpt/{model_name}_epoch_{epoch}_lr_{learning_rate}_batch_{batch_size}_{class_weight}_best_{today}.keras",
    save_best_only=True, monitor="val_auc", mode="max", verbose=1
)
early_stopping = EarlyStopping(monitor="val_auc", patience=5, restore_best_weights=True)
tensorboard_cb = TensorBoard(log_dir=log_dir)
lr_scheduler = ReduceLROnPlateau(monitor="val_auc", factor=0.5, patience=5, min_lr=1e-6)

# âœ… í•™ìŠµ
model.fit(
    X_train_scaled, y_train,
    epochs=epoch,
    batch_size=batch_size,
    validation_split=0.2,
    callbacks=[checkpoint_cb, early_stopping, tensorboard_cb, lr_scheduler],
    verbose=1
)

# âœ… í‰ê°€
y_pred_prob = model.predict(X_test_scaled)
y_pred = (y_pred_prob > 0.5).astype(int).reshape(-1)
print("âœ… í‰ê°€ ê²°ê³¼:\n", classification_report(y_test, y_pred))


# âœ… Accuracy Score ì¶”ê°€ ì¶œë ¥
accuracy = accuracy_score(y_test, y_pred)
print(f"ğŸ¯ ì •í™•ë„ (Accuracy): {accuracy:.4f}")  # âœ… ì¶”ê°€ ì¶œë ¥

# âœ… ì €ì¥
model.save(f"model/{model_name}_epoch_{epoch}_lr_{learning_rate}_batch_{batch_size}_{class_weight}_final_{today}.keras")
print(f"ğŸ“Œ model saved: model/{model_name}_epoch_{epoch}_lr_{learning_rate}_batch_{batch_size}_{class_weight}_final_{today}.keras")
