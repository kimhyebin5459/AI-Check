import os
import random
import numpy as np
import pandas as pd
import tensorflow as tf

from feature_extraction import get_mel_spectrogram_tf, get_vector_features_tf, get_mel_sequence_tf
from preprocess import filter_nan_audio
# from load_files import load_audio_file_paths, load_image_file_paths

# í™˜ê²½ ì„¤ì •
os.environ["CUDA_DEVICE_ORDER"] = ""
# os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
# os.environ["CUDA_VISIBLE_DEVICES"] = "9"

# ê¸°ë³¸ ì„¤ì •
IMG_HEIGHT = 128
IMG_WIDTH = 500
VECTOR_DIM = 10
SEQUENCE_LEN = 400
N_MELS = 128

# ì˜¤ë””ì˜¤ ë° ì´ë¯¸ì§€ ë””ì½”ë”©
def decode_audio(file_path):
    audio_binary = tf.io.read_file(file_path)
    audio, _ = tf.audio.decode_wav(audio_binary, desired_channels=1)
    return tf.squeeze(audio, axis=-1)

def decode_image(image_path):
    image_binary = tf.io.read_file(image_path)
    image = tf.io.decode_png(image_binary, channels=1)
    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])
    image = tf.image.convert_image_dtype(image, tf.float32)
    return image

# ì‹œí€€ìŠ¤ íŒ¨ë”©
def pad_sequence(seq, target_len=SEQUENCE_LEN):
    cur_len = tf.shape(seq)[0]
    if cur_len < target_len:
        pad = tf.zeros([target_len - cur_len, seq.shape[1]])
        return tf.concat([seq, pad], axis=0)
    return seq[:target_len, :]



# âœ… ëŒ€í‘œ ë°ì´í„°ì…‹ êµ¬ì„± í•¨ìˆ˜
def representative_dataset():
    CSV_PATH = "dataset/filtered_merged_paths.csv"
    df = pd.read_csv(CSV_PATH)

    # ğŸ”€ ë¬´ì‘ìœ„ ìƒ˜í”Œ ì„ íƒ (100ê°œ)
    paths = list(zip(df["audio_path"].tolist(), df["image_path"].tolist()))
    random.seed(42)
    random.shuffle(paths)
    sample_paths = paths[:100]

    for audio_path, image_path in sample_paths:
        waveform = decode_audio(audio_path)
        vector = get_vector_features_tf(waveform)
        sequence = get_mel_sequence_tf(waveform)
        sequence = pad_sequence(sequence, target_len=400)

        image = decode_image(image_path, img_width=IMG_WIDTH)

        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        yield [tf.expand_dims(image, 0), tf.expand_dims(vector, 0), tf.expand_dims(sequence, 0)]

# âœ… ëª¨ë¸ ë¡œë“œ ë° TFLite ë³€í™˜
# model_name = "multi_cnn_bigru_epoch_10_batch_32_final_2025-03-27_15-43-13"
model_name = "multi_cnn_bilstm_epoch_10_batch_32_final_2025-03-27_15-42-04"
model_path = f"model/{model_name}.keras"
model = tf.keras.models.load_model(model_path, compile=False)


# TensorFlow Lite Converterë¥¼ ì´ìš©í•œ ì •ìˆ˜ ì–‘ìí™” ì§„í–‰ (ìˆ˜ì •ëœ ë¶€ë¶„)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
# ëŒ€í‘œ ë°ì´í„°ì…‹ ì„¤ì •
converter.representative_dataset = representative_dataset

# Resource ë³€ìˆ˜ ì‚¬ìš© í™œì„±í™”
converter.experimental_enable_resource_variables = True
# TensorList ê´€ë ¨ ì—°ì‚° ë‚´ë¦¼(lowering) ê¸°ëŠ¥ ë¹„í™œì„±í™”
converter._experimental_lower_tensor_list_ops = False

# Flex Ops(SELECT_TF_OPS)ë„ í—ˆìš©í•˜ì—¬ ë³€í™˜ (ì •ìˆ˜ ì–‘ìí™”ì™€ í•¨ê»˜ ì‚¬ìš©)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS_INT8,
    tf.lite.OpsSet.SELECT_TF_OPS
]

# (ì„ íƒ ì‚¬í•­) ì…ë ¥ê³¼ ì¶œë ¥ë„ int8ë¡œ ë³€í™˜ (ëª¨ë°”ì¼ì—ì„œì˜ íš¨ìœ¨ì„±ì„ ìœ„í•´)
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

# ë³€í™˜ ìˆ˜í–‰
tflite_model_full_int = converter.convert()





# ì €ì¥
os.makedirs("quant_model", exist_ok=True)
with open(f"quant_model/{model_name}.tflite", "wb") as f:
    f.write(tflite_model)
print("âœ… ì–‘ìí™” ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
