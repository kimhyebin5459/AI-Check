import os
import random
import numpy as np
import pandas as pd
import tensorflow as tf

# from predict import decode_image
from feature_extraction import get_mel_spectrogram_tf, get_vector_features_tf, get_mel_sequence_tf
from preprocess import filter_nan_audio
# from load_files import load_audio_file_paths, load_image_file_paths

# í™˜ê²½ ì„¤ì •
os.environ["CUDA_DEVICE_ORDER"] = ""
# os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
# os.environ["CUDA_VISIBLE_DEVICES"] = "7"

# ê¸°ë³¸ ì„¤ì •
IMG_HEIGHT = 128
IMG_WIDTH = 500
VECTOR_DIM = 10
SEQUENCE_LEN = 400
N_MELS = 128

# ì˜¤ë””ì˜¤ ë””ì½”ë”©
def decode_audio(file_path):
    audio_binary = tf.io.read_file(file_path)
    audio, _ = tf.audio.decode_wav(audio_binary, desired_channels=1)
    return tf.squeeze(audio, axis=-1)

def decode_image(image_path, width):
    image_binary = tf.io.read_file(image_path)
    image = tf.io.decode_png(image_binary, channels=1)
    image = tf.image.resize(image, [IMG_HEIGHT, width])
    image = tf.image.convert_image_dtype(image, tf.float32)
    
    # í™•ì‹¤í•˜ê²Œ shape ë³´ì • (í•­ìƒ 3Dë¡œ)
    image.set_shape([IMG_HEIGHT, width, 1])

    return image

# ì‹œí€€ìŠ¤ íŒ¨ë”©
# def pad_sequence(seq, target_len=SEQUENCE_LEN):
#     cur_len = tf.shape(seq)[0]
#     if cur_len < target_len:
#         pad = tf.zeros([target_len - cur_len, seq.shape[1]])
#         return tf.concat([seq, pad], axis=0)
#     return seq[:target_len, :]

def pad_sequence(seq, target_len=SEQUENCE_LEN):
    cur_len = tf.shape(seq)[0]
    if cur_len < target_len:
        pad = tf.zeros([target_len - cur_len, seq.shape[1]])
        seq = tf.concat([seq, pad], axis=0)
    else:
        seq = seq[:target_len, :]
    seq.set_shape([SEQUENCE_LEN, 128])  # ğŸ”¥ ëª…ì‹œì  shape ì„¤ì •
    return seq


# âœ… ëŒ€í‘œ ë°ì´í„°ì…‹ êµ¬ì„± í•¨ìˆ˜ (TFLite ì–‘ìí™” ì‹œ ì‚¬ìš©)
def representative_dataset():
    CSV_PATH = "dataset/filtered_merged_paths.csv"
    df = pd.read_csv(CSV_PATH)

    paths = list(zip(df["audio_path"].tolist(), df["image_path"].tolist()))
    random.seed(42)
    random.shuffle(paths)
    sample_paths = paths[:100]

    for audio_path, image_path in sample_paths:
        waveform = decode_audio(audio_path)
        vector = get_vector_features_tf(waveform)
        sequence = get_mel_sequence_tf(waveform)
        sequence = pad_sequence(sequence, target_len=SEQUENCE_LEN)
        sequence.set_shape([SEQUENCE_LEN, 128])  # âœ… ëª…ì‹œ

        image = decode_image(image_path, IMG_WIDTH)
        image.set_shape([IMG_HEIGHT, IMG_WIDTH, 1])  # âœ… ëª…ì‹œ

        # í™•ì‹¤í•˜ê²Œ ë°°ì¹˜ ì°¨ì›ê¹Œì§€ ê³ ì •
        img_input = tf.expand_dims(image, 0)
        vec_input = tf.expand_dims(vector, 0)
        seq_input = tf.expand_dims(sequence, 0)

        img_input.set_shape([1, IMG_HEIGHT, IMG_WIDTH, 1])   # âœ…
        vec_input.set_shape([1, 10])                         # âœ…
        seq_input.set_shape([1, SEQUENCE_LEN, 128])          # âœ…

        print("image:", img_input.shape)
        print("vector:", vec_input.shape)
        print("sequence:", seq_input.shape)

        yield [img_input, vec_input, seq_input]

# def representative_dataset():
#     CSV_PATH = "dataset/filtered_merged_paths.csv"
#     df = pd.read_csv(CSV_PATH)

#     # ë¬´ì‘ìœ„ ìƒ˜í”Œ ì„ íƒ (ì˜ˆ: 100ê°œ)
#     paths = list(zip(df["audio_path"].tolist(), df["image_path"].tolist()))
#     random.seed(42)
#     random.shuffle(paths)
#     sample_paths = paths[:100]
#     for audio_path, image_path in sample_paths:
#         waveform = decode_audio(audio_path)
#         vector = get_vector_features_tf(waveform)
#         sequence = get_mel_sequence_tf(waveform)
#         sequence = pad_sequence(sequence, target_len=SEQUENCE_LEN)
    
#         image = decode_image(image_path, IMG_WIDTH)
    
#         img_input = tf.expand_dims(image, 0)
#         vec_input = tf.expand_dims(vector, 0)
#         seq_input = tf.expand_dims(sequence, 0)
    
#         print("image:", img_input.shape)
#         print("vector:", vec_input.shape)
#         print("sequence:", seq_input.shape)
    
#         yield [img_input, vec_input, seq_input]

#     # for audio_path, image_path in sample_paths:
#     #     waveform = decode_audio(audio_path)
#     #     vector = get_vector_features_tf(waveform)
#     #     sequence = get_mel_sequence_tf(waveform)
#     #     sequence = pad_sequence(sequence, target_len=SEQUENCE_LEN)

#     #     image = decode_image(image_path, IMG_WIDTH)
#     #     # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ (ê° input shapeê°€ ëª¨ë¸ê³¼ ì¼ì¹˜í•´ì•¼ í•¨)
#     #     yield [
#     #         tf.expand_dims(image, 0),      # (1, 128, 500, 1)
#     #         tf.expand_dims(vector, 0),     # (1, 10)
#     #         tf.expand_dims(sequence, 0)    # (1, 400, 128)
#     #     ]

# âœ… ëª¨ë¸ ë¡œë“œ ë° TFLite ë³€í™˜
model_name = "multi_cnn_bigru_epoch_10_batch_32_final_2025-03-27_15-43-13"
model_path = f"model/{model_name}.keras"
model = tf.keras.models.load_model(model_path, compile=False)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = []
# converter.optimizations = [tf.lite.Optimize.DEFAULT]
# converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]

converter.representative_dataset = representative_dataset
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,   # ê¸°ë³¸ TFLite ops
    tf.lite.OpsSet.SELECT_TF_OPS        # Flex ops ì‚¬ìš© í—ˆìš©
]
converter._experimental_lower_tensor_list_ops = False
converter.experimental_enable_resource_variables = True

tflite_model = converter.convert()

# ì €ì¥
os.makedirs("quant_model", exist_ok=True)
with open(f"quant_model/{model_name}.tflite", "wb") as f:
    f.write(tflite_model)
print("âœ… ì–‘ìí™” ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")





converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.allow_custom_ops = True
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
# ì…ë ¥ shapeì„ -1ë¡œ ì§€ì •í•˜ë©´ dynamic batch
converter.experimental_new_converter = True
tflite_model = converter.convert()