{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e01079-68d3-4a04-a3a0-bc3149ecd26c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1743953106.139622 1883371 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2025-04-06 15:25:06.141116: W tensorflow/core/framework/op_kernel.cc:1857] OP_REQUIRES failed at conv_ops_impl.h:1204 : INVALID_ARGUMENT: No DNN in stream executor.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'conv1' (type Conv1D).\n\n{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:GPU:0}} No DNN in stream executor. [Op:Conv2D] name: \n\nCall arguments received by layer 'conv1' (type Conv1D):\n  • inputs=tf.Tensor(shape=(1, 3001, 128), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m model_dir = \u001b[33m\"\u001b[39m\u001b[33mwhisper-large-v3\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# 사용자의 모델 디렉토리 경로로 변경\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 2. PyTorch 체크포인트를 기반으로 TensorFlow 모델로 변환 (from_pt=True)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m tf_model = \u001b[43mTFWhisperForConditionalGeneration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_pt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTensorFlow 모델이 로드되었습니다.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# 3. SavedModel 형식으로 저장 (TFLite 변환에 필요)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:2963\u001b[39m, in \u001b[36mTFPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[39m\n\u001b[32m   2960\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_pytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_pytorch_checkpoint_in_tf2_model\n\u001b[32m   2962\u001b[39m     \u001b[38;5;66;03m# Load from a PyTorch checkpoint\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2963\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_pytorch_checkpoint_in_tf2_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2964\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2965\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2966\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2967\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2968\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_weight_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtf_to_pt_weight_rename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf_to_pt_weight_rename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2970\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2972\u001b[39m \u001b[38;5;66;03m# we might need to extend the variable scope for composite models\u001b[39;00m\n\u001b[32m   2973\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load_weight_prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/modeling_tf_pytorch_utils.py:208\u001b[39m, in \u001b[36mload_pytorch_checkpoint_in_tf2_model\u001b[39m\u001b[34m(tf_model, pytorch_checkpoint_path, tf_inputs, allow_missing_keys, output_loading_info, _prefix, tf_to_pt_weight_rename)\u001b[39m\n\u001b[32m    204\u001b[39m     pt_state_dict.update(state_dict)\n\u001b[32m    206\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPyTorch checkpoint contains \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(t.numel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mt\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mpt_state_dict.values())\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m parameters\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_pytorch_weights_in_tf2_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtf_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpt_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtf_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtf_to_pt_weight_rename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf_to_pt_weight_rename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/modeling_tf_pytorch_utils.py:252\u001b[39m, in \u001b[36mload_pytorch_weights_in_tf2_model\u001b[39m\u001b[34m(tf_model, pt_state_dict, tf_inputs, allow_missing_keys, output_loading_info, _prefix, tf_to_pt_weight_rename)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# Numpy doesn't understand bfloat16, so upcast to a dtype that doesn't lose precision\u001b[39;00m\n\u001b[32m    249\u001b[39m pt_state_dict = {\n\u001b[32m    250\u001b[39m     k: v.numpy() \u001b[38;5;28;01mif\u001b[39;00m v.dtype != torch.bfloat16 \u001b[38;5;28;01melse\u001b[39;00m v.float().numpy() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m pt_state_dict.items()\n\u001b[32m    251\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_pytorch_state_dict_in_tf2_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtf_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpt_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtf_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtf_to_pt_weight_rename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf_to_pt_weight_rename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/modeling_tf_pytorch_utils.py:326\u001b[39m, in \u001b[36mload_pytorch_state_dict_in_tf2_model\u001b[39m\u001b[34m(tf_model, pt_state_dict, tf_inputs, allow_missing_keys, output_loading_info, _prefix, tf_to_pt_weight_rename, ignore_mismatched_sizes, skip_logger_warnings)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tf_inputs:\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tf.name_scope(_prefix):\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m         \u001b[43mtf_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Make sure model is built\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[38;5;66;03m# Convert old format to new format if needed from a PyTorch state_dict\u001b[39;00m\n\u001b[32m    328\u001b[39m tf_keys_to_pt_keys = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/siuenv/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:437\u001b[39m, in \u001b[36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    434\u001b[39m     config = \u001b[38;5;28mself\u001b[39m.config\n\u001b[32m    436\u001b[39m unpacked_inputs = input_processing(func, config, **fn_args_and_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/whisper/modeling_tf_whisper.py:1435\u001b[39m, in \u001b[36mTFWhisperForConditionalGeneration.call\u001b[39m\u001b[34m(self, input_features, decoder_input_ids, decoder_attention_mask, decoder_position_ids, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[39m\n\u001b[32m   1430\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1431\u001b[39m         decoder_input_ids = shift_tokens_right(\n\u001b[32m   1432\u001b[39m             labels, \u001b[38;5;28mself\u001b[39m.config.pad_token_id, \u001b[38;5;28mself\u001b[39m.config.decoder_start_token_id\n\u001b[32m   1433\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1435\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1436\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1442\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1445\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1446\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1447\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1448\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1450\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1451\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1452\u001b[39m decoder_last_hidden_state = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1453\u001b[39m \u001b[38;5;66;03m# Decoder and encoder embeddings are tied\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:437\u001b[39m, in \u001b[36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    434\u001b[39m     config = \u001b[38;5;28mself\u001b[39m.config\n\u001b[32m    436\u001b[39m unpacked_inputs = input_processing(func, config, **fn_args_and_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/whisper/modeling_tf_whisper.py:1166\u001b[39m, in \u001b[36mTFWhisperMainLayer.call\u001b[39m\u001b[34m(self, input_features, decoder_input_ids, decoder_attention_mask, decoder_position_ids, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[39m\n\u001b[32m   1163\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m     encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a TFBaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[32m   1175\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, TFBaseModelOutput):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:437\u001b[39m, in \u001b[36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    434\u001b[39m     config = \u001b[38;5;28mself\u001b[39m.config\n\u001b[32m    436\u001b[39m unpacked_inputs = input_processing(func, config, **fn_args_and_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/whisper/modeling_tf_whisper.py:760\u001b[39m, in \u001b[36mTFWhisperEncoder.call\u001b[39m\u001b[34m(self, input_features, head_mask, output_attentions, output_hidden_states, return_dict, training)\u001b[39m\n\u001b[32m    758\u001b[39m input_features = tf.transpose(input_features, perm=(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m    759\u001b[39m input_features = tf.pad(input_features, [[\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m], [\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m], [\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m]])\n\u001b[32m--> \u001b[39m\u001b[32m760\u001b[39m inputs_embeds = keras.activations.gelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    761\u001b[39m inputs_embeds = tf.pad(inputs_embeds, [[\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m], [\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m], [\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m]])\n\u001b[32m    762\u001b[39m inputs_embeds = keras.activations.gelu(\u001b[38;5;28mself\u001b[39m.conv2(inputs_embeds))\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: Exception encountered when calling layer 'conv1' (type Conv1D).\n\n{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:GPU:0}} No DNN in stream executor. [Op:Conv2D] name: \n\nCall arguments received by layer 'conv1' (type Conv1D):\n  • inputs=tf.Tensor(shape=(1, 3001, 128), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFWhisperForConditionalGeneration\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "\n",
    "# 1. 모델 디렉토리 설정\n",
    "# 이 디렉토리에는 config.json, pytorch_model.bin, generation_config.json,\n",
    "# tokenizer_config.json, special_tokens_map.json, preprocessor_config.json, normalizer.json, merges.txt, vocab.json, added_tokens.json 등이 포함되어 있어야 합니다.\n",
    "model_dir = \"whisper-large-v3\"  # 사용자의 모델 디렉토리 경로로 변경\n",
    "\n",
    "# 2. PyTorch 체크포인트를 기반으로 TensorFlow 모델로 변환 (from_pt=True)\n",
    "tf_model = TFWhisperForConditionalGeneration.from_pretrained(model_dir, from_pt=True)\n",
    "print(\"TensorFlow 모델이 로드되었습니다.\")\n",
    "\n",
    "# 3. SavedModel 형식으로 저장 (TFLite 변환에 필요)\n",
    "saved_model_dir = \"./tf_whisper_saved\"\n",
    "tf.saved_model.save(tf_model, saved_model_dir)\n",
    "print(\"SavedModel이 저장되었습니다:\", saved_model_dir)\n",
    "\n",
    "# 4. TFLite 변환\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "\n",
    "# 최적화를 활성화하여 모델 경량화 (포스트 트레이닝 양자화 등)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# (선택 사항) 대표 데이터셋을 제공하여 양자화 보정을 진행할 수 있습니다.\n",
    "# 아래는 대표 데이터셋 함수의 예시입니다. 실제 데이터셋을 사용하면 더 좋습니다.\n",
    "# def representative_dataset():\n",
    "#     for _ in range(100):\n",
    "#         # preprocessor_config.json에 따라 입력은 (1, 128, 3000) 형태로 예상됨\n",
    "#         dummy_input = np.random.rand(1, 128, 3000).astype(np.float32)\n",
    "#         yield [dummy_input]\n",
    "# converter.representative_dataset = representative_dataset\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# 5. TFLite 모델 파일로 저장\n",
    "tflite_model_file = \"whisper-large-v3.tflite\"\n",
    "with open(tflite_model_file, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"TFLite 모델이 성공적으로 저장되었습니다:\", tflite_model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e608d4a-b811-43f8-aed3-f4733eb3b280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b5103d-4aa8-43e4-8c0d-2457c89e5627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c99afa-64f4-4b9f-a691-bb274a46cf73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (siuenv)",
   "language": "python",
   "name": "siuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
